{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a convolutional neural network to classify glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "data_root = os.path.join('.', 'notMNIST_data') # Change me to store data elsewhere\n",
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory ./notMNIST_data/logs_v7 does not exist. Creating it...\n",
      "Created directory ./notMNIST_data/logs_v7.\n",
      "Directory ./notMNIST_data/save_v7 does not exist. Creating it...\n",
      "Created directory ./notMNIST_data/save_v7.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "log_dir = os.path.join(data_root, 'logs_v7')\n",
    "if not os.path.exists(log_dir):\n",
    "    print('Directory %s does not exist. Creating it...' % log_dir)\n",
    "    os.makedirs(log_dir)\n",
    "    print('Created directory %s.' % log_dir)\n",
    "    \n",
    "save_dir = os.path.join(data_root, 'save_v7')\n",
    "if not os.path.exists(save_dir):\n",
    "    print('Directory %s does not exist. Creating it...' % save_dir)\n",
    "    os.makedirs(save_dir)\n",
    "    print('Created directory %s.' % save_dir)\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    with tf.name_scope('input'):\n",
    "        input_data = tf.placeholder(\n",
    "            tf.float32, shape=(None, image_size, image_size, num_channels),\n",
    "            name='input_data')\n",
    "        input_labels = tf.placeholder(tf.float32, shape=(None, num_labels),\n",
    "            name='input_labels')\n",
    "        tf.summary.image('input', input_data, 10)\n",
    "\n",
    "    # Variables.\n",
    "    def weight_variable(shape):\n",
    "        \"\"\"Create a weight variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "        return tf.Variable(initial)\n",
    "\n",
    "    def bias_variable(shape):\n",
    "        \"\"\"Create a bias variable with appropriate initialization.\"\"\"\n",
    "        initial = tf.constant(0.1, shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def variable_summaries(var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "            with tf.name_scope('stddev'):\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "            tf.summary.scalar('max', tf.reduce_max(var))\n",
    "            tf.summary.scalar('min', tf.reduce_min(var))\n",
    "            tf.summary.histogram('histogram', var)\n",
    "            \n",
    "    def conv_layer(input_tensor, patch_size, input_channels, output_channels, layer_name, act=tf.nn.relu):\n",
    "        \"\"\"Reusable code for making a simple neural net layer.\n",
    "        It does a 2d convolution, bias add, and then uses ReLU to nonlinearize.\n",
    "        It also sets up name scoping so that the resultant graph is easy to read,\n",
    "        and adds a number of summary ops.\n",
    "        \"\"\"\n",
    "        # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "        with tf.name_scope(layer_name):\n",
    "            # This Variable will hold the state of the weights for the layer\n",
    "            with tf.name_scope('weights'):\n",
    "                weights = weight_variable([patch_size, patch_size, input_channels, output_channels])\n",
    "                variable_summaries(weights)\n",
    "            with tf.name_scope('biases'):\n",
    "                biases = bias_variable([output_channels])\n",
    "                variable_summaries(biases)\n",
    "            with tf.name_scope('conv_plus_b'):\n",
    "                preactivate = tf.nn.conv2d(input_tensor, weights, [1, 1, 1, 1], padding='SAME') + biases\n",
    "                tf.summary.histogram('pre_activations', preactivate)\n",
    "            activations = act(preactivate, name='activation')\n",
    "            tf.summary.histogram('activations', activations)\n",
    "            return activations\n",
    "        \n",
    "    def pool_layer(input_tensor, layer_name):\n",
    "        \"\"\"Reusable code for making a simple max pooling layer\n",
    "        \"\"\"\n",
    "        # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "        with tf.name_scope(layer_name):\n",
    "            pool = tf.nn.max_pool(input_tensor, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "            return pool\n",
    "    \n",
    "    def nn_layer(input_tensor, input_dim, output_dim, layer_name, act=tf.nn.relu):\n",
    "        \"\"\"Reusable code for making a simple neural net layer.\n",
    "        It does a matrix multiply, bias add, and then uses ReLU to nonlinearize.\n",
    "        It also sets up name scoping so that the resultant graph is easy to read,\n",
    "        and adds a number of summary ops.\n",
    "        \"\"\"\n",
    "        # Adding a name scope ensures logical grouping of the layers in the graph.\n",
    "        with tf.name_scope(layer_name):\n",
    "            # This Variable will hold the state of the weights for the layer\n",
    "            with tf.name_scope('weights'):\n",
    "                weights = weight_variable([input_dim, output_dim])\n",
    "                variable_summaries(weights)\n",
    "            with tf.name_scope('biases'):\n",
    "                biases = bias_variable([output_dim])\n",
    "                variable_summaries(biases)\n",
    "            with tf.name_scope('Wx_plus_b'):\n",
    "                preactivate = tf.matmul(input_tensor, weights) + biases\n",
    "                tf.summary.histogram('pre_activations', preactivate)\n",
    "            activations = act(preactivate, name='activation')\n",
    "            tf.summary.histogram('activations', activations)\n",
    "            return activations\n",
    "    \n",
    "    # Model.\n",
    "    conv1 = conv_layer(input_data, patch_size, num_channels, depth, 'conv1')\n",
    "    pool1 = pool_layer(conv1, 'pool1')\n",
    "    conv2 = conv_layer(pool1, patch_size, depth, depth, 'conv2')\n",
    "    pool2 = pool_layer(conv2, 'pool2')\n",
    "    \n",
    "    with tf.name_scope('image_reshape'):\n",
    "        shape = pool2.get_shape().as_list()\n",
    "        reshape = tf.reshape(pool2, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "    hidden1 = nn_layer(reshape, image_size // 4 * image_size // 4 * depth, num_hidden, 'hidden1')\n",
    "    hidden2 = nn_layer(hidden1, num_hidden, num_hidden, 'hidden2')\n",
    "    \n",
    "    with tf.name_scope('dropout'):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        tf.summary.scalar('dropout_keep_probability', keep_prob)\n",
    "        dropped = tf.nn.dropout(hidden2, keep_prob)\n",
    "    \n",
    "    logits = nn_layer(dropped, num_hidden, num_labels, 'hidden3', act=tf.identity)\n",
    "\n",
    "    # Training computation.\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        diff = tf.nn.softmax_cross_entropy_with_logits(labels=input_labels, logits=logits)\n",
    "        with tf.name_scope('total'):\n",
    "            cross_entropy = tf.reduce_mean(diff)\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "            cross_entropy)\n",
    "\n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(input_labels, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Merge all the summaries\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Accuracy at step 0: 0.0845\n",
      "Adding run metadata for 99\n",
      "Accuracy at step 100: 0.692\n",
      "Adding run metadata for 199\n",
      "Accuracy at step 200: 0.7766\n",
      "Adding run metadata for 299\n",
      "Accuracy at step 300: 0.8135\n",
      "Adding run metadata for 399\n",
      "Accuracy at step 400: 0.8165\n",
      "Adding run metadata for 499\n",
      "Accuracy at step 500: 0.819\n"
     ]
    }
   ],
   "source": [
    "num_steps = 12501 # 12,501 steps takes an hour or two on my laptop\n",
    "k = 0.5 # keep probability\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    train_writer = tf.summary.FileWriter(log_dir + '/train', sess.graph)\n",
    "    valid_writer = tf.summary.FileWriter(log_dir + '/valid')\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        if step % 100 == 0:  # Record summaries and valid-set accuracy\n",
    "            # TODO: Set up feed_dict for validation set\n",
    "            feed_dict = {input_data : valid_dataset, input_labels : valid_labels, keep_prob : 1.0}\n",
    "            summary, acc = sess.run([merged, accuracy], feed_dict=feed_dict)\n",
    "            valid_writer.add_summary(summary, step)\n",
    "            print('Accuracy at step %s: %s' % (step, acc))\n",
    "        if step % 100 == 0:\n",
    "            saver.save(sess, save_dir + '/my-model-new', global_step=step)\n",
    "        # Record train set summaries, and train\n",
    "        if step % 100 == 99:  # Record execution stats\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {input_data : batch_data, input_labels : batch_labels, keep_prob : k}\n",
    "            summary, _ = sess.run([merged, train_step],\n",
    "                                  feed_dict=feed_dict,\n",
    "                                  options=run_options,\n",
    "                                  run_metadata=run_metadata)\n",
    "            train_writer.add_run_metadata(run_metadata, 'step%03d' % step)\n",
    "            train_writer.add_summary(summary, step)\n",
    "            print('Adding run metadata for', step)\n",
    "        else:  # Record a summary\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {input_data : batch_data, input_labels : batch_labels, keep_prob : k}\n",
    "            summary, _ = sess.run([merged, train_step], feed_dict=feed_dict)\n",
    "            train_writer.add_summary(summary, step)\n",
    "    train_writer.close()\n",
    "    valid_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
